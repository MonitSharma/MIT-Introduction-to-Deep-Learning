{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT Intro to Deep Learning Lecture 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrk9UOHMcj9/pJA5lhO0Dx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonitSharma/MIT-Introduction-to-Deep-Learning/blob/main/MIT_Intro_to_Deep_Learning_Lecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHPmmAGtjLXS"
      },
      "source": [
        "Install Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpgslnp8gnoh"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjyyYX1U_s65"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE2htE0E_0Gv"
      },
      "source": [
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def _init__(self, input_dim , output_dim):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "\n",
        "    #initialize weights and biases\n",
        "    self.W = self.add_weight([input_dim, output_dim])\n",
        "    self.b = self.add_weight([1, output_dim])\n",
        "\n",
        "  def call(self,inputs):\n",
        "    # forward propagation\n",
        "    z = tf.matmul(inputs, self.W) + self.b\n",
        "\n",
        "    # non linear activation\n",
        "\n",
        "    output = tf.math.sigmoid(z)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPFI0PonAYJF"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJ417LxBlyN"
      },
      "source": [
        "Tensorflow can do all this with just one line of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeZep_apAo_8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "layer = tf.keras.layers.Dense(units=2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8uS0L0FCz1G"
      },
      "source": [
        "Now we Create A Single Layer Neural Network, The first layer with 5 neurons and the output layer with 2 neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoEBs_OaBzn-"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(5), tf.keras.layers.Dense(2)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9305zuhDefs"
      },
      "source": [
        "More the layers, deeper the network, and we'll add more and more line of Dense tf with the number of Neurons specified.\n",
        "\n",
        "Here I've coded a neural network with 4 input layer and 1 output layer, \n",
        "where the first three input layers have 5 neurons each , and the fourth input layer has four neurons. The Output layer has two neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay0_4QLlDLSE"
      },
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(5),\n",
        "                             tf.keras.layers.Dense(5), tf.keras.layers.Dense(5),\n",
        "                             tf.keras.layers.Dense(4),tf.keras.layers.Dense(2)])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iFHmayEHztQ"
      },
      "source": [
        "Binary Cross Entropy Loss\n",
        "\n",
        "Here Y is the actual Output, and predicted is the one , which our neural network Tell us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1o2FXPmJqKe"
      },
      "source": [
        "We are Using Binary cross entropy loss, because we are answering a a question with two outputs.\n",
        "\n",
        "Given the data of students, where \\\\\n",
        "$x_1$ = number of lectures they attend \\\\\n",
        "$x_2$ = number of hours they give on the proeject \\\\\n",
        "Give predictions, that a specific student will pass or fail?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-YIEvRlD-Kp"
      },
      "source": [
        "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(y,predicted))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OdCB1GrKKBS"
      },
      "source": [
        "If the above mentioned question , changes to \n",
        "Given the data of students, where \\\\\n",
        "$x_1$ = number of lectures they attend \\\\\n",
        "$x_2$ = number of hours they give on the proeject \\\\\n",
        "make a neural network , that predicts the grade of the student"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Y-zsJeKcaS"
      },
      "source": [
        "Mean Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xGRYkvWKlmH"
      },
      "source": [
        "loss = tf.reduce_mean( tf.square(tf.subtract(y,predicted)))\n",
        "loss = tf.keras.losses.MSE(y, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aY9d24kK8kG"
      },
      "source": [
        "Gradient Descent Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVZ_RkZ9Uh8V"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "weights = tf.Variable([tf.random.normal()])\n",
        "\n",
        "\n",
        "while(True):\n",
        "  with tf.GradientTape() as g:\n",
        "    loss = compute_loss(weights)\n",
        "    gradient = g.gradient(loss, weights)\n",
        "\n",
        "  weights = weights - lr* gradient"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGVSiZd8eO3r"
      },
      "source": [
        "Types of Gradient Descent Algorithms\n",
        "<li>SGD\n",
        "<li> Adam\n",
        "<li> Adadelta\n",
        "<li> Adagrad\n",
        "<li> RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjzFkd0LU_qm",
        "outputId": "1838682b-75fc-41a4-8ab5-630f6c308c5e"
      },
      "source": [
        "tf.keras.optimizers.SGD\n",
        "tf.keras.optimizers.Adam\n",
        "tf.keras.optimizers.Adadelta\n",
        "tf.keras.optimizers.Adagrad\n",
        "tf.keras.optimizers.RMSprop"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QENUQ9Sezzp"
      },
      "source": [
        "Putting all of the Above codes together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2nEJ6J9ekN9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(5),\n",
        "                             tf.keras.layers.Dense(5), tf.keras.layers.Dense(5),\n",
        "                             tf.keras.layers.Dense(4),tf.keras.layers.Dense(2)])\n",
        "\n",
        "optimizer = tf.keras.optimizer.SGD()\n",
        "\n",
        "while True:\n",
        "  # loop forever\n",
        "  # forward pass through the network\n",
        "  prediction = model(x)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # compute the loss\n",
        "    loss = ompute_loss(y, prediction)\n",
        "\n",
        "  # update the weights\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMIx35rh0mx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFCIHO-QhoLJ"
      },
      "source": [
        "To avoid overfitting, we use Regularization techniques\n",
        "<li> Dropout\n",
        "\n",
        "in the code, the p*100  is the percentage of neurons dropped in the activation layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDBSZCGjhvZl"
      },
      "source": [
        "tf.keras.layers.Dropout(p=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}