{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wqxW97BDky2"
      },
      "source": [
        "## Text Generation with RNNs\n",
        "\n",
        "How to generate text using a character based RNN, using Shakespeare's writings.\n",
        "\n",
        "Given a sequence  of characters from the data, train a model to predict the next character in the sequence.\n",
        "\n",
        "\n",
        "Some sentences are grammatically correct, but the others do not make sense \n",
        "<li> The model is character-based. When training started, the model did not know how to spell an ENglish word, or that words were even a unit of text.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsqXmwtACfw7",
        "outputId": "b0bc151c-6d62-45f6-f8fc-5ad40ebd5344"
      },
      "source": [
        "# import tensorflow and the dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czb4k_B_Dq1l"
      },
      "source": [
        "#### Reading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Wu3pKuCho-",
        "outputId": "338700af-8665-4324-dfc5-b4ad31cb5a97"
      },
      "source": [
        "text = open(path_to_file,'rb').read().decode(encoding = 'utf-8')\n",
        "\n",
        "print(f'Length of the text: {len(text)} characters')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoWogh9UCjM2",
        "outputId": "cfcea2f4-74bb-4570-9f0c-bb70b418923b"
      },
      "source": [
        "# take a look at first 1000 letters\n",
        "print(text[:1000])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ri_nxWMCksb"
      },
      "source": [
        "# how many unique characters\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "#vocab"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoXQeMizCmS_"
      },
      "source": [
        "# how many unique characters\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "#vocab"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVkCiChiCxA-",
        "outputId": "9dc7f54c-4b32-4124-d17e-ee85b5400b05"
      },
      "source": [
        "print(f'{len(vocab)} unique characters')\n",
        "\n",
        "# the unique characters consist the alphabets captial and small and punctuations"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkqByDB_Dve_"
      },
      "source": [
        "### Vectorize the Text\n",
        "\n",
        "Before training, we need to convert the strings to a numerical representation\n",
        "\n",
        "We'll use \"preprocessing.StringLookup\" layer can convert each character into a numeric ID\n",
        "\n",
        "We just need to split texts into token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kV51ePqCzUu",
        "outputId": "fc4b294e-1a73-4550-9ac5-57de168daf1a"
      },
      "source": [
        "example_texts = ['Monit', 'Sharma']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'M', b'o', b'n', b'i', b't'], [b'S', b'h', b'a', b'r', b'm', b'a']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQMh6dRoC06t",
        "outputId": "8a17966f-d7a6-48b8-969c-6775c98ed222"
      },
      "source": [
        "# now the preprocessing one\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary= list(vocab), mask_token= None)\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "ids"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[26, 54, 53, 48, 59], [32, 47, 40, 57, 52, 40]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmmF94R1C2CY"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "vocabulary = ids_from_chars.get_vocabulary(), invert = True, mask_token = None)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3P5hLs4C3iR",
        "outputId": "2cc3fe3f-e635-43d1-a881-de8b7f1cc10b"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'M', b'o', b'n', b'i', b't'], [b'S', b'h', b'a', b'r', b'm', b'a']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jgVb1NmC4mR",
        "outputId": "848e2066-bb02-4f26-c7a2-9d905e00e7a0"
      },
      "source": [
        "# make it back as a string\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'Monit', b'Sharma'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XepkSSlwC5zW"
      },
      "source": [
        "# simply make a function out of it\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2BnzZUoD1KK"
      },
      "source": [
        "### The Prediciton Task\n",
        "The input of the model will be a sequence of characters, and we train the model to predict the output- the following characters at each time step.\n",
        "\n",
        "#### Create Training examples and targets\n",
        "\n",
        "Divide the text into example sequences. Each input will contain seq_length characters from the text\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to right.\n",
        "\n",
        "For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "\n",
        "We'll use the 'tf.data.Dataset.from_tensor_slices\" function to convert the text vector into a stream of character indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdremU_EC7Qx",
        "outputId": "960f75bf-469b-4b03-f570-be32499e6711"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uPXAJV0C8nS"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC6VKgnQC943",
        "outputId": "205eb8b4-1335-4f3d-86d4-42d08359c95c"
      },
      "source": [
        "for ids in ids_dataset.take(20):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n",
            "\n",
            "\n",
            "B\n",
            "e\n",
            "f\n",
            "o\n",
            "r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4XTwepC_Jg"
      },
      "source": [
        "# we'll take sequence of 100 char\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDFJ7SPFDAcT",
        "outputId": "3574a13e-047b-43a4-f0cf-b9b1433a947c"
      },
      "source": [
        "# the batch method helps us to convert the individual characters to sequences of desired size\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder= True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0QLZ6yfDBno",
        "outputId": "5f39ace8-adbe-4550-b2c9-0840de09d415"
      },
      "source": [
        "# join them back in strings\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksDSkWj5DDDZ"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bVYMB3DEf7",
        "outputId": "a72b3904-85c6-4399-f9ab-3de31485fea8"
      },
      "source": [
        "split_input_target(list(\"Monit Sharma\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['M', 'o', 'n', 'i', 't', ' ', 'S', 'h', 'a', 'r', 'm'],\n",
              " ['o', 'n', 'i', 't', ' ', 'S', 'h', 'a', 'r', 'm', 'a'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEMOnHnNDFmH"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtJ2EokoDG2s",
        "outputId": "304207f1-09e4-475f-f260-f89edfb355c8"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input: \" ,text_from_ids(input_example).numpy())\n",
        "    print(\"Target: \", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rY6AnXYD8wz"
      },
      "source": [
        "#### Creating trainig batches\n",
        "\n",
        "We need to shuffle the data and put that into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYfCXfyxDIDY",
        "outputId": "279373a2-8f47-470c-d743-bebcf047da1f"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements)\n",
        "\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        ".prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii4GieHhEDLW"
      },
      "source": [
        "### Build the Model\n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "<li>tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
        "<li>tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
        "<lI>tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAgaW7d7DJnI"
      },
      "source": [
        "# length of the vocabulary in hte chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# embedding _ dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# number of RNN units\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luJG1H3ZDK8p"
      },
      "source": [
        "# now the model\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state= True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        \n",
        "    def call(self, inputs, states= None, return_state= False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training= training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "        \n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "   "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C67hY-HdDMDD"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOz-YvseEGZh"
      },
      "source": [
        "### Try the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQhwz53JDNVd",
        "outputId": "05079989-b92b-4d6e-af15-3f8b1e700e14"
      },
      "source": [
        "# checking the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pku_8_PDOZ9",
        "outputId": "0f4d1231-ce62-4f92-ed07-afcff6aa8e81"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygdDv4HYDP7J"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DhPBOIUDRqK",
        "outputId": "699fc95c-1200-4611-f316-48565230529a"
      },
      "source": [
        "# This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17, 25, 54, 20, 14, 16,  7, 22, 31, 25, 43, 62,  6, 50, 15, 19, 31,\n",
              "       28,  5, 17, 44, 60,  9, 23, 48,  7, 58, 13,  1, 38, 40, 21, 26, 59,\n",
              "        9,  1, 34,  9, 11,  5, 42, 56, 56,  4, 43, 14, 63, 43, 53, 12, 39,\n",
              "       31, 47, 27,  8, 63, 36, 50, 19, 56, 42, 35, 60, 23,  0, 27, 29, 27,\n",
              "       25, 31,  8, 25, 22, 23, 61, 30, 57,  8, 64, 31, 31, 32, 43, 32, 51,\n",
              "       46, 62, 28, 64, 10, 45, 26, 36, 58,  7, 23, 42, 18, 49, 23])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xinG3C3DTYO",
        "outputId": "2f2e721f-4b40-40d7-a02e-8298dc8a36e0"
      },
      "source": [
        "# Decode these to see the text predicted by this untrained model:\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'ther circumstances\\nMade up to the deed, doth push on this proceeding:\\nYet, for a greater confirmatio'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"DLoGAC,IRLdw'kBFRO&Deu.Ji,s?\\nYaHMt.\\nU.:&cqq$dAxdn;ZRhN-xWkFqcVuJ[UNK]NPNLR-LIJvQr-yRRSdSlgwOy3fMWs,JcEjJ\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJaOGRykELmS"
      },
      "source": [
        "### Train the Model\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "#### Attach an Optimizer and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XO0gp4NDUvg"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHaOdU22DWOW",
        "outputId": "0c2fe818-134e-41e8-dbb9-03b02bbe416a"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.1890397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpyPOpwNDYIL",
        "outputId": "af9f532f-68e8-441e-b6c6-856bdfb5d2ad"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.95942"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHjcFiaQDZgY"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXazvweAEQFz"
      },
      "source": [
        "### Configure Checkpoints\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS76uOeoDa0V"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktAZuiFKEXfv"
      },
      "source": [
        "### Execute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO8VL9ImDcG3"
      },
      "source": [
        "\n",
        "EPOCHS = 20"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3jK5pw7DdcE",
        "outputId": "dcacccfb-3465-4e2e-de55-f4ef4077f3ae"
      },
      "source": [
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 2.7328\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.9973\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.7206\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.5576\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4563\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3879\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3344\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2886\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2475\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2070\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.1667\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1256\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0824\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0366\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.9893\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 60ms/step - loss: 0.9384\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.8868\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8338\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.7823\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.7326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2CXcnOoEiVl"
      },
      "source": [
        "## generate text\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "\n",
        "Each time we call the model we pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "669f4qz7DfVP"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW8R4PAhEz7u"
      },
      "source": [
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcXHsHl4E5dg"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKWVEq0eE2tc",
        "outputId": "31ab8979-0299-40b0-f6d1-53499ecb72a9"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "O, sir, it is an hunn and odd! These are\n",
            "past three of our commission. Go not succeed.\n",
            "\n",
            "ISABELLA:\n",
            "Ay, but by guess.\n",
            "\n",
            "WARWICK:\n",
            "And I could live and mock nor fram, and in this\n",
            "complaint that makes the mind spoil from hence,\n",
            "More than you think, that dog into the tortures\n",
            "Do execute their headst of her becomes.\n",
            "The mightive hand that is dear partain\n",
            "As I thou wast boy were four asheem again,\n",
            "Remember 'twas I war; but being obedience.\n",
            "And look you, ladies out of noble Bolingbroke\n",
            "Will all spread up to the world that drops of thee,\n",
            "Nay, Women on off from him they have, with brood\n",
            "And many by 'shal, a bait of breath,\n",
            "When I shall you no great account of his sin.\n",
            "One Margaret, but we ordend and lust:\n",
            "Bianco, of as is springs bound to thee and trields,\n",
            "A child, my reason why they have paid\n",
            "The stamp'd-nice breaking onacters.\n",
            "\n",
            "Second Murderer:\n",
            "O God'd my lord; it is a guest friend Wills,\n",
            "And set ochem against the prince my state:\n",
            "A-blow is only in some-planets, that sall you,\n",
            "And how dost thou \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.645212411880493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkOvOeXdFCYD"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try EPOCHS = 30).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd3KqJmWE7uU",
        "outputId": "5414d315-13b0-4abb-9802-0f2bba367095"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nFalsely to make the law: there is the man of winds\\nFrom the charp of young and by o'er-heart\\nAnd pass'd the morning rodumen before his\\nsweet bybutish two my heart; and when the heart they saw he married\\nWere frozand of a cunning law: may it hath\\nbanied, young bridegroom from Parrish centurius.\\nTo him the tomb.\\n\\nANTIGOUNE:\\nmark you.\\n\\nAUTILLIUS:\\nI stand forbid\\nTo have him out o' the clod; thou'rt in mark'd\\nBy so dwelling att, and ne'er the day of beating:\\nNow, no more poth, sir. Pame your sword I saw him from her:\\nYou take your daughter make a knee as Kender: it\\nis the wance of heaven and the blashf\\nLers is their parties, and the hearts to enewe\\nWhere he appears he in the towns; herein we be spoke,\\nWhich contradicts him: in her hope herein with\\nWarwick: this is known that Henry's eyes.\\nThis sensible damned Juliet, who should keep him where now,\\nHath win my all hath been a caseless shun.\\nA bact of you, impartience bladement,\\nThan dignity in foreign scarcely games,\\nHe who shall be but kin\"\n",
            " b\"ROMEO:\\nBy and by my nature when he stands,\\nAll successes an eye; whense's very days,\\nWhilst Death already, yau, my grave, shall thy name?\\n\\nVOLUMNIA:\\nHoptessio here, this shall in terms\\nBecome that is his enemies.\\n\\nSLY:\\nMy heart for that, make war wast thou back to make\\nYou throught the starf; but still the tribunes on my blessing.\\nKnine I thee, if he is hine, Bragenet,\\nRichmond in heart-clapp's men that fly,\\nThe men is made to bed, with pearlest! pretending this,\\nI shall be possible that the unevidence with\\nA bogion of the son of kings,\\nAre joundy of poor Hatharina tell her, but to-morrow\\nThen preserver so in much Juliet.\\n\\nFirst Keeper:\\nForward, I promise, I'll rent their friends;\\nIn them by me are all my land with one: thou\\nart the music, nor down, go fetch both point,\\nAnd I'll believe thee.\\n\\nLEONTES:\\nWhen I was in the present, place, incagable!\\nO heaven, my mother, you may march.\\n\\nGREMIO:\\nThe boy there is an apples in the Green, and surdeed,\\nAll music from blood were rival-hablied.\\nWhich b\"\n",
            " b\"ROMEO:\\nGood equalo,\\nOur cordial comfort have he igher labour here\\nStands not to do another bustler o' the stalm!\\n\\nLADY ANNE:\\nGod give your graces both, and know her mind\\nThat I am nature; no, and Northumberland,\\nI smill thee well; which I do lose thee mar.\\n\\nBAPTISTA:\\nHow fares our care! Bolingbroke, hath it fall in thus;\\nIn that hath been the horse in hands farewell.\\n\\nPETRUCHIO:\\n\\nProvost:\\nIs it hath done thee here, that's a wench\\nfast, which is enough? Is there was smoting forth\\nFrom eirs that by our office of heaven.\\n\\nVIRGILIA:\\nHenry, hear me not, let me now mean it yours.\\n\\nFirst Lord:\\nPeace, peace!'\\n\\nMENENIUS:\\nWell, we'll deliver you this a thousand times to come when thou hast\\nCreentiely; and it beggars here all:\\nMy provertis, worms, he is a record to\\ndo in the tomb.\\n\\nISABELLA:\\nI prithee here it is a gentleman that you deart.\\n\\nJULIET:\\nAs much as many distression supsesses begins;\\nWhilst he seem seen twenty thousand men\\nDeceive his limitange, or whither shall go walk.\\n\\nHASTINGS:\\nMy lord, y\"\n",
            " b\"ROMEO:\\nO ignorant, in the one Ifalloy.\\n\\nCLADENCE:\\nTherefore you come to prison! lower me\\nTo this jest wash'd bound in the court?\\nAnd when thou hast done earn'd before him,\\nIs all to prick my purpose. What\\nTame thou, thou damn'd-work!\\n\\nFLORIZEL:\\nHe is; and so!\\nAnd thou, too all, and cry our ear; 'tis tire\\nHen reverence for mink be a poison,\\nAnd bown by marriage meth my brother's house,\\nSo many good patiful Clarence; 'ay, then stood up;\\nFor, look you, he was never grown bolent, bound to itself the\\nshadows of my thim by the wreck. You guess they ha?\\n\\nBUCKINGHAM:\\nWell, ha! by my consent Roman, Drusberise, and\\nSirrah Grumio, never woe did end:\\nEven now him over that hath sunged. Spread from thither\\nHath but O command, my heart to fight with sighs;\\nFor Marcius early now can but Hereford be\\nTo like a giet for corrorn game to face,\\nWe'll send you only of your honour served in their\\narmour.\\n\\nProvost:\\nNow, Welthou blood! see thou dismissed with our blood\\nFor fearful arm'd for concerns to him and passi\"\n",
            " b\"ROMEO:\\nFear not watcher, with care, for Lucio, do I gued\\nwith limish two go warm'd, the rest if you\\nHow to have holp to raise me yield the shrow.\\n\\nCAPULET:\\nDo so boy corruct it, to her convert by\\nthe shadows of Oxford, that it may change thee:\\nI have no cause to every doof convey again\\nIn that would here with art these themes\\nAnd be their just. O children,\\nStand by thy damned thunder! Why, when it doth remiga\\nService this simple publict on\\nthe night in pleasure that word 'I reso me.\\nYet shall this remorse man is to wail it twenty\\nHis curning in the use on his will.\\n\\nCAPULET:\\nBut she is pass, and when they shall know't;\\nI thank you for the unto them!\\n\\nLADY GREY:\\nSo did he me: this boint evil hearts\\nAgainst the Duke of Blace come, you are so amiss;\\nYet nature bidding from your butcher's kinsmen,\\nI swear by any reprove in that rarent\\nThat I may live to dare the senation.\\n\\nBAPTISTA:\\nHow in your pratiness in two subjects' wants\\nIf I unwithout despection, look on\\nReed from her horse.\\nNay, that she\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.6944024562835693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAlwiknvFGpj"
      },
      "source": [
        "### Export the generator\n",
        "This single-step model can easily be saved and restored, allowing you to use it anywhere a tf.saved_model is accepted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr7qXcrLFEin",
        "outputId": "8e1ba581-c41c-46d9-e030-fe86a01d56ee"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fc1dc0acf50>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXGwziz4FLUs",
        "outputId": "63b9338e-ac4b-4401-f776-a2011d4686bc"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Go loud unto your majesty: thou hast dinner\n",
            "More than her countenance; and 'twere first\n",
            "Sent for th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoKW5uhlFPiS"
      },
      "source": [
        "### Advanced: Customize Training\n",
        "The above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement curriculum learning to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use tf.GradientTape to track the gradients. You can learn more about this approach by reading the eager execution guide.\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "Execute the model and calculate the loss under a tf.GradientTape.\n",
        "Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKVn-Q09FM6F"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoIQvEaIFXps"
      },
      "source": [
        "The above implementation of the train_step method follows Keras' train_step conventions. This is optional, but it allows you to change the behavior of the train step and still use keras' Model.compile and Model.fit methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf_vZtjrFV8P"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcwxCXUZFY57"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsyPoQXJFaMd",
        "outputId": "ef28fc57-bade-4653-bd52-47087a2a2ee9"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 13s 58ms/step - loss: 2.7266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1d6c48b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqRzYxSFFbOb",
        "outputId": "84769746-d712-4a2d-a384-7302753a16cd"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1852\n",
            "Epoch 1 Batch 50 Loss 2.0757\n",
            "Epoch 1 Batch 100 Loss 1.9578\n",
            "Epoch 1 Batch 150 Loss 1.8748\n",
            "\n",
            "Epoch 1 Loss: 1.9947\n",
            "Time taken for 1 epoch 11.64 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8493\n",
            "Epoch 2 Batch 50 Loss 1.7206\n",
            "Epoch 2 Batch 100 Loss 1.6861\n",
            "Epoch 2 Batch 150 Loss 1.6450\n",
            "\n",
            "Epoch 2 Loss: 1.7139\n",
            "Time taken for 1 epoch 10.71 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5871\n",
            "Epoch 3 Batch 50 Loss 1.5880\n",
            "Epoch 3 Batch 100 Loss 1.5619\n",
            "Epoch 3 Batch 150 Loss 1.5737\n",
            "\n",
            "Epoch 3 Loss: 1.5509\n",
            "Time taken for 1 epoch 10.38 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4670\n",
            "Epoch 4 Batch 50 Loss 1.4482\n",
            "Epoch 4 Batch 100 Loss 1.4608\n",
            "Epoch 4 Batch 150 Loss 1.4165\n",
            "\n",
            "Epoch 4 Loss: 1.4517\n",
            "Time taken for 1 epoch 10.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3690\n",
            "Epoch 5 Batch 50 Loss 1.3747\n",
            "Epoch 5 Batch 100 Loss 1.3952\n",
            "Epoch 5 Batch 150 Loss 1.3674\n",
            "\n",
            "Epoch 5 Loss: 1.3833\n",
            "Time taken for 1 epoch 10.21 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3122\n",
            "Epoch 6 Batch 50 Loss 1.3188\n",
            "Epoch 6 Batch 100 Loss 1.3161\n",
            "Epoch 6 Batch 150 Loss 1.3234\n",
            "\n",
            "Epoch 6 Loss: 1.3300\n",
            "Time taken for 1 epoch 10.25 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2823\n",
            "Epoch 7 Batch 50 Loss 1.3177\n",
            "Epoch 7 Batch 100 Loss 1.2930\n",
            "Epoch 7 Batch 150 Loss 1.3546\n",
            "\n",
            "Epoch 7 Loss: 1.2861\n",
            "Time taken for 1 epoch 10.38 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2249\n",
            "Epoch 8 Batch 50 Loss 1.2368\n",
            "Epoch 8 Batch 100 Loss 1.2425\n",
            "Epoch 8 Batch 150 Loss 1.2702\n",
            "\n",
            "Epoch 8 Loss: 1.2444\n",
            "Time taken for 1 epoch 10.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1883\n",
            "Epoch 9 Batch 50 Loss 1.2042\n",
            "Epoch 9 Batch 100 Loss 1.1965\n",
            "Epoch 9 Batch 150 Loss 1.2363\n",
            "\n",
            "Epoch 9 Loss: 1.2046\n",
            "Time taken for 1 epoch 10.50 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1517\n",
            "Epoch 10 Batch 50 Loss 1.1764\n",
            "Epoch 10 Batch 100 Loss 1.1353\n",
            "Epoch 10 Batch 150 Loss 1.1607\n",
            "\n",
            "Epoch 10 Loss: 1.1656\n",
            "Time taken for 1 epoch 10.54 sec\n",
            "________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}